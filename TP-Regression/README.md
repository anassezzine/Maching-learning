**Rapport du TP : R√©gression Lin√©aire et R√©gularisation**

## **1. Introduction**

L'objectif de ce TP est d'impl√©menter et d'analyser diff√©rentes techniques de r√©gression lin√©aire,
notamment la r√©gression par moindres carr√©s, Ridge et Lasso, en utilisant `numpy` et `scikit-learn`.
Nous allons comparer leurs performances en termes d'erreur quadratique moyenne et d'impact sur les coefficients.

## **2. R√©gression Lin√©aire par Moindres Carr√©s**

L'algorithme impl√©ment√© suit la formule analytique :
\[
w = (X^TX)^{-1} X^T y
\]
Nous avons appliqu√© cette r√©gression sur un jeu de donn√©es `dataRegLin2D.txt`, et obtenons les r√©sultats suivants :

- **Coefficients obtenus :**
  - Moindres Carr√©s : `[-0.678, 0.245, 1.370]`
  - Scikit-learn : `[0.245, 1.370]`

- **Erreur quadratique moyenne (MSE) :**
  - Moindres Carr√©s : `0.0102`
  - Scikit-learn : `0.0102`

Les r√©sultats sont tr√®s similaires entre l'impl√©mentation manuelle et celle de `scikit-learn`, confirmant ainsi la validit√© de notre code.

## **3. R√©gression Ridge et Lasso**

La r√©gression Ridge et Lasso ajoutent une p√©nalisation sur les coefficients pour r√©duire le sur-apprentissage :
- **Ridge** : Ajoute une p√©nalisation L2 (`\lambda \|w\|^2`)
- **Lasso** : Ajoute une p√©nalisation L1 (`\lambda \|w\|`), ce qui peut forcer certains coefficients √† 0.

Nous avons test√© ces m√©thodes en optimisant le param√®tre `alpha` avec `GridSearchCV`.
Les meilleurs `alpha` trouv√©s sont :
- Ridge : `0.001`
- Lasso : `0.00207`

**Comparaison des erreurs :**
- Moindres carr√©s : `0.0102`
- Ridge : `0.0102`
- Lasso : `0.0103`

**Impact sur les coefficients :**
- Ridge : les coefficients sont tr√®s proches de la r√©gression classique.
- Lasso : certains coefficients sont r√©duits plus fortement, ce qui favorise la s√©lection de variables.

## **4. Analyse Graphique**

Nous avons visualis√© les r√©sultats avec :
1. **Graphiques 3D** : Comparaison des valeurs pr√©dites et des valeurs r√©elles.
2. **Impact de la r√©gularisation sur l'erreur** : L'erreur varie en fonction de `alpha`.
3. **√âvolution des coefficients** : Lasso r√©duit plus fortement les coefficients que Ridge.

## **5. Conclusion**

- La r√©gression par moindres carr√©s fonctionne bien pour ce jeu de donn√©es.
- Ridge et Lasso offrent des avantages en termes de g√©n√©ralisation et de s√©lection de variables.
- L'optimisation du param√®tre `alpha` est cruciale pour de meilleurs r√©sultats.

Ce TP a permis de mieux comprendre les diff√©rentes techniques de r√©gression lin√©aire et leur impact sur les donn√©es. üöÄ

